{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Learning\\git\n"
     ]
    }
   ],
   "source": [
    "!cd .."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-10T14:18:17.221475Z",
     "end_time": "2023-07-10T14:18:17.237127Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-10T14:18:22.834295Z",
     "end_time": "2023-07-10T14:18:24.952900Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.vectorstores import Milvus\n",
    "import os\n",
    "from langchain.callbacks import wandb_tracing_enabled\n",
    "\n",
    "\n",
    "from constants import (EMBEDDING_MODEL_NAME, MODEL_ID, MODEL_BASENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-10T14:18:25.833387Z",
     "end_time": "2023-07-10T14:18:25.850756Z"
    }
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s - %(filename)s:%(lineno)s - %(message)s\",\n",
    "                    level=logging.INFO)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-10T14:18:26.196281Z",
     "end_time": "2023-07-10T14:18:26.216124Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_cpu_model(model_id, model_basename):\n",
    "    model_path = hf_hub_download(repo_id=model_id, filename=model_basename)\n",
    "\n",
    "    # Callbacks support token-wise streaming\n",
    "    callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "    # Verbose is required to pass to the callback manager\n",
    "\n",
    "    model = LlamaCpp(model_path=model_path, n_ctx=2048, max_tokens=2048, temperature=0, repeat_penalty=1.15,\n",
    "                     callback_manager=callback_manager, verbose=True)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-10T14:18:26.423381Z",
     "end_time": "2023-07-10T14:18:36.358233Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-10 14:18:31,038 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: hkunlp/instructor-large\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-10 14:18:35,076 - INFO - 2111476880.py:3 - embedding model loaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_seq_length  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n",
      "2023-07-10 14:18:36,339 - INFO - 2111476880.py:7 - text generator model loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load the embedding model\n",
    "embeddings = HuggingFaceInstructEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={\"device\": 'cuda'})\n",
    "logging.info('embedding model loaded.')\n",
    "\n",
    "# Load text generator model\n",
    "llm = load_cpu_model(MODEL_ID, MODEL_BASENAME)\n",
    "logging.info('text generator model loaded.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-10T14:18:36.362233Z",
     "end_time": "2023-07-10T14:18:36.386229Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_llm_generation_langchain(question):\n",
    "    vector_store = Milvus(embedding_function=embeddings,\n",
    "                          connection_args={\"host\": \"localhost\", \"port\": \"19530\"},\n",
    "                          collection_name='PAN',\n",
    "                          index_params={\"metric_type\": \"IP\", \"params\": {\"nprobe\": 10}})\n",
    "\n",
    "    qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vector_store.as_retriever(),\n",
    "                                     return_source_documents=True)\n",
    "\n",
    "    # Get the answer from the chain\n",
    "    res = qa(question)\n",
    "    answer, docs = res[\"result\"], res[\"source_documents\"]\n",
    "\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-10T14:18:36.376232Z",
     "end_time": "2023-07-10T14:18:36.394232Z"
    }
   },
   "outputs": [],
   "source": [
    "# W&B env setup\n",
    "os.environ[\"LANGCHAIN_WANDB_TRACING\"] = \"true\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"langchain-testing\"\n",
    "\n",
    "# unset the environment variable and use a context manager instead\n",
    "if \"LANGCHAIN_WANDB_TRACING\" in os.environ:\n",
    "    del os.environ[\"LANGCHAIN_WANDB_TRACING\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-10T14:18:36.391231Z",
     "end_time": "2023-07-10T14:20:20.990689Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Streaming LangChain activity to W&B at https://wandb.ai/aditya/langchain-testing/runs/cavseyhs\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: `WandbTracer` is currently in beta.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Please report any issues to https://github.com/wandb/wandb/issues with the tag `langchain`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " What is the purpose of a PAN card?"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello! How can I assist you today?"
     ]
    }
   ],
   "source": [
    "# enable tracing using a context manager\n",
    "with wandb_tracing_enabled():\n",
    "    while True:\n",
    "        query = input(\"\\nEnter a query: \")\n",
    "        if query == \"exit\":\n",
    "            break\n",
    "        # Get the answer from the chain\n",
    "        answer = get_llm_generation_langchain(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-10T14:20:20.972691Z",
     "end_time": "2023-07-10T14:20:20.991689Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
